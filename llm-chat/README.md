# LLM Chat Interface

A simple chat interface for interacting with Large Language Models (LLMs) through Ollama.

## Features

- ğŸš€ Real-time streaming responses
- ğŸ¨ Web interface using Streamlit
- ğŸ”§ Temperature control
- ğŸ“ Chat history
- ğŸ”„ Multiple model support

## Quick Start

1. Install dependencies:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

2. Configure Ollama host in `.env`:
```
OLLAMA_HOST=http://your-ollama-host:11434
```

3. Run the app:
```bash
streamlit run main.py
```

4. Open in browser:
- http://localhost:8501

## Project Structure
```
llm_chat/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/          # Ollama client
â”‚   â”œâ”€â”€ utils/        # Helper functions
â”œâ”€â”€ config/           # Settings
â”œâ”€â”€ data/            # Chat history
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py
```

## Requirements

- Python 3.11+
- Ollama running locally or remotely