# config/config.yaml
llm:
  host: "http://172.16.12.102:11434"  # Default Ollama host
  temperature: 0.5
  max_tokens: 2048
  model_name: "llama3.2:3b"

redis:
  host: "localhost"
  port: 6379
  db: 0
  max_retries: 3
  retry_interval: 1

debug: false